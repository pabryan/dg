#+TITLE: Inverse Function Theorem
#+OPTIONS: toc:nil num:nil

* Topology on \(\RR^n\)
** Open and closed balls

#+BEGIN_env defn
Given \(r > 0\) and \(x \in \RR^n\), the /open ball/ of radius \(r\) and centre \(x\) is the set
\[
B_r (x) = \{y \in \RR^n: \abs{x - y} < r\}.
\]

The /closed ball/ of radius \(r\) and centre \(x\) is the set
\[
\bar{B}_r (x) = \{y \in \RR^n: \abs{x - y} \leq r\}.
\]
#+END_env

** Spheres

#+BEGIN_env defn

The /sphere/ of radius \(r\) and centre \(x\) is the set
\[
\SS^{n-1}_r (x) = \{y \in \RR^n: \abs{x - y} = r\}.
\]
#+END_env

** Distance function

#+BEGIN_env defn
For \(x, y \in \RR^n\), the distance \(\abs{x - y}\) is defined to be
\[
\abs{x - y} = \sqrt{(x^1 - y^1)^2 + \cdots + (x^n - y^n)^2}
\]
where \(x = (x^1, \dots, x^n)\) and \(y = (y^1, \dots, y^n)\).
#+END_env

** Distance function

- The open ball is the set of points of distance to \(x\) strictly less than \(r\).
- The closed ball is the set of points of distance to \(x\) less than or equal to \(r\).
- The sphere is the set of points of distance to \(x\) equal to \(r\).

It is sometimes said that analysis is simply applications of the triangle inequality:
\[
\abs{x - y} \leq \abs{x - z} + \abs{z - y}.
\]

** Open and closed sets

#+BEGIN_env defn
A set \(U \subset \RR^n\) is said to be /open/ provided for every \(x \in U\), there exists an \(r = r(x)\) such that
\[
B_r(x) \subseteq U.
\]


A set \(C\) is /closed/ if it's complement,
\[
\RR^n \backslash C := \{y \in \RR^n : y \notin C\}
\]
is open.
#+END_env

** Open and closed sets

- By this definition, open balls are open, closed balls are closed and spheres are closed.
- Given any point of an open set, we can always move /uniformly/ a little in any direction and remain in the open set.

** Bounded and compact sets

#+BEGIN_env defn
A set \(S \subseteq \RR^n\) is /bounded/ if there exists an \(x \in \RR^n\) and an \(r > 0\) such that \(S \subseteq B_r(x)\).

A set \(K \subseteq \RR^n\) is /compact/ if it is closed and bounded.
#+END_env

** Bounded and compact sets

- \(S \subseteq \RR^n\) is bounded iff for every \(x \in \RR^n\) there exists an \(r = r(x)\) such that \(S \subseteq B_r(x)\).
- A set \(K \subseteq \RR^n\) is compact if and only if for every /open cover/ \(\{U_{\alpha}\}\), there exists a /finite subcover/.
  - An /open cover/ is a collection of open sets \(\{U_{\alpha}\}\) such that \(K \subseteq \cup_{\alpha} U_{\alpha}\).
  - A /finite subcover/ is a cover by finitely many \(U_{\alpha_1}, \cdots, U_{\alpha_N}\)
  - Equivalent for \(\RR^n\).

* Limits and continuity
** Limits

#+BEGIN_env defn
A sequence \((x_n)_{n\in \NN} \subseteq \RR^n\) /converges/ to \(x \in \RR^n\) if for every \(\epsilon > 0\), there exists a \(N \in \NN\) such that \((x_n)_{n \geq N} \subseteq B_{\epsilon} (x)\). We write \(\lim_{n\to\infty} x_n = x\).
#+END_env

** Cauchy Sequences

#+BEGIN_env defn
The sequence \((x_n)\) is /Cauchy/ if for every \(\epsilon > 0\), there exists a \(N \in \NN\) such that \((x_m)_{m \geq N} \subseteq B_{\epsilon} (x_n)\) for every \(n \geq N\).
#+END_env

** Limits and Cauchy Sequences

- The condition for convergence to \(x\) says that \(\abs{x - x_n} < \epsilon\) for \(n \geq N\).
- The condition to be a Cauchy sequence says that \(\abs{x_n - x_m} < \epsilon\) for \(m, n \geq N\).

** Sequential Continuity

#+BEGIN_env defn :title "Sequential definition"
A function \(f : \RR^n \to \RR^m\) is continuous at \(x \in \RR^n\) if for /every/ sequence \((x_n)\) with \(\lim_{n \to \infty} = x\) we have \(\lim_{n \to \infty} f(x_n) = f(x)\).
#+END_env

** Epsilon-Delta Continuity

#+BEGIN_env defn :title "Epsilon-Delta definition"
Write
\[
\lim_{x \to x_0} f(x) = y
\]
provided for every \(\epsilon > 0\), there exists a \(\delta > 0\) such that \(f(B_{\delta} (x_0)) \subseteq B_{\epsilon} (y)\).

Then \(f\) is continuous at \(x_0\) if \(\lim_{x \to x_0} f(x) = f(x_0)\).
#+END_env

** Topological Continuity


#+BEGIN_env defn :title "Topological definition"
The function \(f\) is continuous (at every \(x_0\)) if \(f^{-1} (V)\) is an open set for every open set \(V \subseteq \RR^m\).
#+END_env

** Continuity Remarks

- The first definition requires that \(f(x_n) \to f(x)\) for /every/ sequence.
- The condition in the second definition that \(f(B_{\delta} (x_0)) \subseteq B_{\epsilon} (y)\) is the same thing as \(\abs{f(x) - f(x_0)} < \epsilon\) whenever \(\abs{x - x_0} < \delta\).
- The second definition says that given /any tolerance \(\epsilon > 0\)/, there is an /adjustment \(\delta > 0\)/ so that provided we are sufficiently close to \(x_0\) (i.e. \(\abs{x - x_0} < \delta\)), then \(f(x)\) is within the desired tolerance of \(f(x_0)\) (i.e. \(\abs{f(x) - f(x_0)} < \epsilon\).

** Continuity Remarks

- The equivalence of the first and second definitions is a standard exercise in analysis using the /completeness/ of the real numbers \(\RR\).
- The final definition is the general /topological/ definition.
- The equivalence of the topological and \(\epsilon\)-\(\delta\) definitions follows by writing \(U = \cup_{y \in U} B_{r(y)} (y)\) as a union of open balls and using properties of the pull-back \(f^{-1}\).

** A cautionary example

Let
\[
f(x, y) = \begin{cases}
\frac{x^2 y}{x^4 + y^2}, \quad (x, y) \ne (0, 0) \\
0, \quad (x, y) = (0, 0).
\end{cases}
\]


Then \(f\) is *not* continuous at \((x, y) = (0, 0)\).

#+BEGIN_notes
However, along every straight line through the origin \(y = ax\), the limit is in fact \(0\)! That is,
\[
\lim_{t \to 0} f(t, at) = \lim_{t\to 0} \frac{t^2 \cdot at}{t^4 + a^2t^2} = \lim_{t\to 0} \frac{t^2}{t^2} \frac{at}{t^2 + a^2} = 0.
\]

But along the curve \(y = x^2\), we get something else:
\[
\lim_{t \to 0} f(t, t^2) = \lim_{t\to 0} \frac{t^2 \cdot t^2}{t^4 + (t^2)^2} = \lim_{t\to 0} \frac{t^4}{t^4} \frac{1}{2} = \frac{1}{2}.
\]
#+END_notes

* Differentiability                                                :noexport:
** Partial derivatives

#+BEGIN_defn
The \(i\)'th /partial derivative/ of a function \(f : \RR^n \to \RR\) at \(x = (x^1, \dots, x^n)\) is
\[
\partial_i f (x) = \frac{\partial f}{\partial x^i} (x) = \lim_{h\to 0} \frac{f(x^1, \dots, x^{i-1}, x^i + h, x^{i+1}, \dots x^n) - f(x^1, \dots, x^n)}{h}.
\]
whenever the limit exists.
#+END_defn


The partial derivative is simply the usual derivative of a function of one variable holding all other variables fixed.

** Directional derivatives

#+BEGIN_defn
Let \(X = (X^1, \dots, X^n) \in \RR^n\). The /directional derivative/ \(df_x \cdot X\) of \(f\) at \(x\) in the direction \(X\) is
\[
\partial_X f (x) = \partial_t|_{t=0} f(x + tX) = \lim_{h \to 0} \frac{f(x + hX) - f(x)}{h}.
\]
#+END_defn


The partial derivative is simply the directional derivative with \(X = e_i\) where \(e_i = (0, \dots, 0, 1, 0, \dots, 0)\) with the \(1\) in the \(i\)'th position is the so-called \(i\)'th basis vector.

** The Differential

Recall that Taylor's theorem with remainder states that
\[
f(x) = f(x_0) + f'(x_0) (x-x_0) + R_{x_0} (x)
\]
where
\[
\lim_{x\to x_0} \frac{\abs{R_{x_0} (x)}}{x-x_0} = 0.
\]
We write \(R_{x_0} (x) = \littleo(x)\) as \(x \to x_0\).


#+BEGIN_defn
We say \(f : \RR^n \to \RR^m\) is differentiable at \(x_0\) if there exists a linear map \(L_{x_0} : \RR^n \to \RR^m\) such that
\[
\lim_{x\to x_0} \frac{\abs{f(x) - f(x_0) - L_{x_0} \cdot (x-x_0)}}{\abs{x-x_0}} = 0.
\]
#+END_defn


That is, there exists a linear map written \(L_{x_0} = df_{x_0}\) such that
\[
f(x) = f(x_0) + df_{x_0} \cdot (x-x_0) + \littleo(\abs{x-x_0}), \quad \text{as \(x \to x_0\)}.
\]

** Differentiable imples partial derivatives exist

Let \(f\) be differentiable at \(x_0 = (x_0^1, \dots, x_0^n)\). For \(h \neq 0\), let \(x = (x_0^1, \dots, x_0^{i-1}, x_0^i + h, x_0^{i+1}, \dots, x_0^n) = x_0 + he_i\). We have
\[
\partial_i f(x_0) = \lim_{h\to 0} \frac{f(x_0 + h e_i) - f(x_0)}{h}
\]
provided the limit exists. Differentiability ensures that
\[
0 = \lim_{h\to 0} \abs{\frac{f(x_0 + h e_i) - f(x_0)}{h} - \frac{df_{x_0} \cdot h e_i}{h}}
\]

and hence
\[
\lim_{h\to 0} \frac{f(x_0 + h e_i) - f(x_0)}{h} = \lim_{h \to 0} \frac{1}{h} df_{x_0} \cdot h e_i = df_{x_0} \cdot e_i.
\]
exists.

- *Exercise*: Show that the same argument proves \(\partial_X f (x_0) = df_{x_0} (X)\) exists.
** A cautionary example

Let
\[
f(x, y) = \begin{cases}
\frac{xy}{x^2 + y^2}, & (x, y) \ne (0, 0) \\
0, & (x, y) = (0, 0).
\end{cases}
\]


Notice that
\[
\partial_x f (0, 0) = \partial_t|_{t=0} f(t, 0) = \partial_t|_{t=0} \frac{t \cdot 0}{t^2 + 0^2} = 0.
\]

Likewise \(\partial_y f(0, 0) = 0\).

However,
\[
\partial_{(1, 1)} f (0, 0) = \partial_t|_{t=0} f(t, t) = \lim_{t\to 0} \frac{1}{t} (f(t, t) - f(0, 0))
\]
is undefined since \(f(t, t) = t^2/(t^2 + t^2) = 1/2\).

Defining, \(f(0, 0) = 1/2\) doesn't help because then \(\partial_{(1, 2)} f (0, 0)\) doesn't exist. In fact, \(f\) is not even continuous at \((0, 0)\).

** \(C^1\) functions

#+BEGIN_defn
A function \(f : \RR^n \to \RR^m\) is \(C^1\) (i.e. has continuous derivative) if \(f\) is differentiable at each \(x\) and moreover, the map
\[
x \mapsto df_x
\]
is continuous. This is equivalent to having /continuous/ partial derivatives.
#+END_defn


Note here that \(df_x\) is a linear map \(\RR^n \to \RR^m\) and the set of all these is linearly isomorphic to the space \(M_{n, m}\) of \(n\) by \(m\) matrices, which is itself linearly isomorphic to \(\RR^{nm}\) (index by \(i,j\) with \(1 \leq i \leq n\) and \(1 \leq j \leq m\)).

Concretely we may realise \(df_x\) as the matrix
\[
(df_x)_{ij} = \partial_i f^j (x) \quad \text{since} \quad df_x \cdot e_i = \partial_i f (x) = (\partial_i f^1, \dots, \partial_i f^n).
\]


Then \(df : \RR^n \to \RR^{nm}\) is a map between Euclidean spaces so we can ask if it's differentiable. Then \(f\) is \(C^2\) if \(d^2 f\) is \(C^1\) and more generally, \(f\) is \(C^k\) if \(d^k f\) is continuous.

** Chain Rule

#+BEGIN_theorem
[Chain Rule]
The chain rule states that if \(f : \RR^n \to \RR^m\) is differentiable at \(x_0\) and \(g : \RR^m \to \RR^k\) is differentiable at \(f(x_0)\), then
\[
d(f \circ h)_{x_0} = dh_{f(x_0)} \cdot df_{x_0}.
\]
#+END_theorem


By the /chain rule/, given any curve \(\gamma\) such that \(\gamma(0) = x\) and \(\gamma'(0) = X\) we have
\[
df_x \cdot X = \partial_t|_{t=0} f(\gamma(t)).
\]


In other words, to compute \(\partial_X f (x)\) we may replace the curve \(t \mapsto x + t X\) with any other curve such that \(\gamma(0) = x\) and \(\gamma'(0) = X\).
* Inverse and Implicit Function Theorems                           :noexport:
** One Dimensional Inverse Function Theorem

#+BEGIN_theorem
Let \(f : \RR \to \RR\) be a smooth function with \(f'(x_0) \ne 0\), there exists an interval \(I\) containing \(x_0\) and an interval \(J\) containing \(f(x_0)\) so that \(f : I \to J\) is a diffeomorphism. That is, there exists an inverse function \(f^{1} : J \to I\). Moreover, for all \(y \in J\),
\[
(f^{-1})' (y) = \frac{1}{f'(f^{-1} (y))}.
\]
#+END_theorem


- To be explicit, the definition of \(f^{-1}\) means that \(f \circ f^{-1} (y) = y)\) for all \(y \in J\) and \(f^{-1} \circ f (x) = x\) for all \(x \in I\).
- In this case, observe that if \(h : J \to \RR\) is a smooth function, then so too is \(h \circ f\). This defines the /pull-back/
  \[
  f^{\ast} : h \in C^{\infty}(J, \RR) \mapsto h \circ f \in C^{\infty} (I, \RR).
  \]
 
- *Exercise*: Show that \(f^{\ast}\) is a bijection with inverse \((f^{-1})^{\ast}\).

** Contraction mappings and fixed points

#+BEGIN_defn
A map \(T : \bar{B}_r(p) \to \bar{B}_r(p)\) is a /contraction map/ if there exists a constant \(0 \leq L < 1\) such that
\[
\abs{T(x) - T(y)} \leq L \abs{x - y}.
\]
#+END_defn

#+BEGIN_theorem
[Banach fixed point theorem]

Let \(T\) be a contraction map. Then there exists a unique /fixed point/ \(x^{\ast} \in B_r(p)\) of \(T\). That is, there exists a unique point \(x^{\ast}\) such that \(T(x^{\ast}) = x^{\ast}\).
#+END_theorem

** Proof of contraction mapping theorem (Uniqueness)

#+BEGIN_proof
\let\qed\relax
Fundamental contraction identity:

\begin{align*}
\abs{x - y} &\leq \abs{x - T(x)} + \abs{T(x) - y} \\
&\leq \abs{x - T(x)} + \abs{T(x) - T(y)} + \abs{T(y) - y} \\
&\leq \abs{x - T(x)} + L \abs{x - y} + \abs{T(y) - y}.
\end{align*}


Therefore
\[
\abs{x - y} \leq \frac{\abs{x - T(x)} + \abs{T(y) - y}}{1-L}
\]


Thus we obtain /uniqueness/: if \(T(x) = x\) and \(T(y) = y\), then \(\abs{x - y} \leq 0\) and hence \(x = y\).
#+END_proof

** Proof of contraction mapping theorem (Existence)

#+BEGIN_proof
\let\qed\relax
Pick any \(x_0\) and define \(x_n = T^n(x_0) = \underbrace{T \circ \cdots \circ T}_{n \text{ times}}  (x_0)\)

The claim is that \(x^{\ast} = \lim_{n\to\infty} x_n\) exists and is the desired fixed point.

Supposing first that the limit exists, then using \(x_n = T(x_{n-1})\) we have
\[
x_{\ast} = \lim_{n\to\infty} x_n = \lim_{n\to\infty} T(x_{n-1}) = T(\lim_{n\to\infty} x_{n-1}) = T(x^{\ast})
\]
where we pass the limit through \(T\) since a contraction mapping is continuous (for any \(\epsilon\) choose \(\delta = \epsilon/L\)).
#+END_proof

** Proof of contraction mapping theorem (Existence)

#+BEGIN_proof
To prove that \(x_n = T^n(x_0)\) has a limit we prove it's a Cauchy sequence. By the fundamental contraction identity
\begin{align*}
\abs{T^n(x_0) - T^m(x_0)} &\leq \frac{\abs{T(T^n(x_0)) - T^n(x_0)} + \abs{T(T^m(x_0)) - T^m(x_0)}}{1-L} \\
&= \frac{\abs{T^n(T(x_0)) - T^n(x_0)} + \abs{T^m(T(x_0) - T^m(x_0)}}{1-L} \\
&\le \frac{L^n \abs{T(x_0) - x_0} + L^m \abs{T(x_0) - x_0}}{1-L} \\
&= \frac {L^n + L^m} {1-L} \abs{T(x_0) - x_0} \to 0 \quad \text{as \(n,m \to \infty\)}.
\end{align*}


Here we used that \(0 \leq L < 1\) and by induction that (*exercise!*)
\[
\abs{T^n(x) - T^n(y)} \leq L^n \abs{x - y}.
\]
#+END_proof

** Inverse Function Theorem

#+BEGIN_theorem
Let \(f : \RR^n \to \RR^n\) a smooth function such that \(df_{x_0}\) is invertible at \(x_0\). Then there is an open set \(U\) containing \(x_0\) and an open set \(V\) containing \(f(x_0)\) such that \(f|U : U \to V\) is a diffeomorphism. Moreover
\[
df^{-1}_{f(x_0)} = (df_{x_0})^{-1}.
\]
#+END_theorem


#+BEGIN_remark
Notice that if \(f\) is a diffeomorphism, then \(f^{-1} \circ f (x) = x\). That is, \(f^{-1} \circ f = \Id_x\). Since \(d\Id_x = \Id_n\), by the chain rule we have
\[
\Id_n = d\Id_x = d(f^{-1} \circ f)_{x_0} = df^{-1}_{f(x_0)} \cdot df_{x_0}.
\]


That is \(df_{x_0}\) is invertible and
\[
(df_{x_0})^{-1} = df^{-1}_{f(x_0)}.
\]
#+END_remark

** Inverse Function Theorem: Idea

#+BEGIN_proof
\let\qed\relax
Here's the basic idea: By definition, we have
\[
f(x) = f(x_0) + df_{x_0} \cdot (x - x_0) + \littleo(|x-x_0|).
\]


Ignoring the error term for the moment, by assumption since \(df_{x_0}\) is invertible, we can solve /uniquely/ for \(x\):
\[
f(x) = f(x_0) + df_{x_0} \cdot (x - x_0) \quad \Rightarrow \quad x = x_0 + df_{x_0}^{-1} (f(x) - f(x_0)).
\]


Write \(y = f(x)\) and \(y_0 = f(x_0)\). Since \(y\) uniquely determines \(x\) we may write \(x = f^{-1}(y)\) and
\[
f^{-1}(y) = f^{-1}(y_0) + df_{x_0}^{-1} \cdot (y - y_0).
\]

So we need to deal with the error terms.
#+END_proof

** Inverse Function Theorem: Contraction

#+BEGIN_proof
\let\qed\relax
We use the contraction mapping theorem: Define for each fixed \(y\),
\[
T_y (x) = x - df_{x_0}^{-1} (f(x) - y).
\]


Then since \(f\) is \(C^1\), so too is \(T\) (dropping the \(y\) subscript for convenience) and
\[
dT_{x_0} = d\Id_{x_0} - df_{x_0}^{-1} df_{x_0} = 0.
\]


By continuity of \(dT\), there exists an open neighbourhood \(U\) of \(x_0\) such that \(\|dT_{x_0}\| \leq 1/2\). That is, for \(x \in U\) and \(X \in \RR^n\),
\[
\abs{dT_{x} \cdot X} \leq \frac{1}{2} \abs{X}.
\]
#+END_proof

** Inverse Function Theorem: Contraction

#+BEGIN_proof
\let\qed\relax
From \(\abs{dT_{x} \cdot X} \leq \frac{1}{2} \abs{X}\), and the mean value inequality, we obtain
\[
\abs{T(x_1) - T(x_2)} \leq \frac{1}{2}\abs{x_1 - x_2}
\]
so that \(T\) is \emph{contractive} for \(x_1, x_2 \in U\).


In order to conclude that \(T\) has a unique fixed point, we need to verify that there is an \(r > 0\) such that \(T : \bar{B}_r(x_0) \to \bar{B}_r(x_0)\).


Since \(x_0 \in U\) and \(U\) is open, there exists an \(r > 0\) such that 
\(B_r(x_0) \subseteq U\).
#+END_Proof

** Inverse function theorem: Contraction

#+BEGIN_proof
\let\qed\relax

Now we restrict the range of possible \(y\): Let \(y_0 = f(x_0)\) and \(y \in B_s(y_0)\) with \(s\) any number satisfying
\[
0 < s < \frac{1-L}{\|df_{x_0}^{-1}\|} r.
\]


Then for \(x \in B_r(x_0)\), recalling \(T(x) = x - df_{x_0}^{-1}(f(x) - y)\) we have
\begin{align*}
\abs{T(x) - x_0} &\leq \abs{T(x) - T(x_0)} + \abs{T(x_0) - x_0} \\
&\leq L\abs{x-x_0} + \abs{-df_{x_0}^{-1}(f(x_0) - y)} \\
&\leq L\abs{x-x_0} + \|df_{x_0}^{-1}\| \abs{y_0 - y} \\
&\leq r L + \|df_{x_0}^{-1}\|s \\
&\leq r L + (1-L)r = r.
\end{align*}


#+END_proof

** Inverse function theorem: Fixed Point

#+BEGIN_proof
\let\qed\relax
That is \(T(x) \in \bar{B}_{x_0} (r)\) for \(x \in \bar{B}_{x_0} (r)\) and \(y \in \bar{B}_s (y_0)\).

Thus for any \(y \in \bar{B}_s(x_0)\), \(T_y : \bar{B}_r (x_0) \to \bar{B}_r (x_0)\) is a contraction mapping, hence:

For each such \(y\), there exists a unique fixed point \(x^{\ast}_y \in \bar{B}_r (x_0)\). That is
\[
x^{\ast}_y = T_y(x^{\ast}_y) = x^{\ast}_y - df_{x_0}^{-1} (f(x^{\ast}_y) - y).
\]


Cancelling \(x^{\ast}_y\) from both sides and since \(df_{x_0}^{-1}\) is non-singular,
\[
df_{x_0}^{-1} (f(x^{\ast}_y) - y) = 0 \Rightarrow f(x^{\ast}_y) = y.
\]

#+END_proof

** Inverse function theorem: Continuity of Inverse

#+BEGIN_proof
\let\qed\relax
*We have finally found our inverse function:* \(f^{-1} (y) = x^{\ast}_y\) for \(y \in B_s(y_0)\). Note we need to restrict the range of \(x\) to the open set \(f^{-1}(B_s(y_0)) \cap B_r(x_0)\) so that \(f\) maps this set into \(B_s(y_0)\).


Since \(T\) is a contraction
\[
\abs{x_1 - x_2 - df_{x_0}^{-1}(f(x_1) - f(x_2))} = \abs{T(x_1) - T(x_2)} \leq L \abs{x_1 - x_2}.
\]


By the /reverse triangle inequality/
\[
\abs{x_1 - x_2} - \abs{df_{x_0}^{-1}(f(x_1) - f(x_2))} \leq L \abs{x_1 - x_2}.
\]


That is,
\[
\abs{x_1 - x_2} \leq \frac{\abs{df_{x_0}^{-1}(f(x_1) - f(x_2))}}{1 - L} \leq \frac{\|df_{x_0}^{-1}\|}{1 - L} \abs{f(x_1) - f(x_2)}.
\]
#+END_proof

** Inverse function theorem: Continuity of Inverse

#+BEGIN_proof
\let\qed\relax

We have
\[
\abs{x_1 - x_2} \leq \frac{\|df_{x_0}^{-1}\|}{1 - L} \abs{f(x_1) - f(x_2)}.
\]


Letting \(y_i = f(x_i)\) so that \(x_i = f^{-1}(y_i)\) gives continuity (even Lipschitz):
\[
\abs{f^{-1}(y_1) - f^{-1}(y_2)} \leq \frac{\|df_{x_0}^{-1}\|}{1 - L}\abs{y_1 - y_2}.
\]


Lipschitz is almost differentiable but not quite (e.g. \(f(x) = |x|\)).
#+END_proof

** Inverse function theorem: Differentiability

#+BEGIN_proof
\let\qed\relax
Pick any arbitrary \(y \in B_s(y_0)\) and any \(h\) such that \(y + h \in B_s(y_0)\), say \(h \in B_{\epsilon} (0)\) so that \(y + h \in B_{\epsilon} (y) \subseteq B_s(y_0)\).

Let \(x = f^{-1} (y)\) and define
\[
R = f^{-1} (y + h) - f^{-1} (y) - df_{x}^{-1} \cdot h.
\]


We need to show that
\[
\lim_{h\to 0} \frac{\abs{R}}{\abs{h}} = 0.
\]
#+END_proof

** Inverse function theorem: Differentiability

#+BEGIN_proof
\let\qed\relax

Let \(k = f^{-1}(y + h) - f^{-1} (y)\) so that \(h = f(x + k) - f(x)\). Then

\begin{align*}
R &= f^{-1} (y + h) - f^{-1} (y) - df_{x}^{-1} \cdot h \\
&= k - df_x^{-1} (f(x + k) - f(x)) \\
&= k - df_x^{-1}(df_x k + \littleo(k)) \\
&= -df_x^{-1} (\littleo(k)).
\end{align*}
#+END_proof

** Inverse function theorem: Differentiability

#+BEGIN_proof
\let\qed\relax

Since \(f^{-1}\) is Lipschitz, with constant \(M\) say, we have
\[
\abs{k} = \abs{f^{-1}(y + h) - f^{-1}(y)} \leq M \abs{y + h - y} = M \abs{h}.
\]


Therefore,
\[
\frac{\abs{R}}{\abs{h}} \leq \|df_x^{-1}\| \frac{\littleo(k)}{\abs{h}} \leq M \|df_x^{-1}\| \frac{\littleo(k)}{\abs{k}}.
\]


The right hand side goes to zero as \(h \to 0\) since \(\abs{k} \leq M \abs{h}\) implies \(k \to 0\) and then by definition of \(\littleo(k)\).
#+END_proof

** Inverse function theorem: Higher regularity

#+BEGIN_proof
\let\qed\relax
So to summarise we have shown the existence of a differentiable local inverse \(f^{-1}\) to \(f\) with differential
\[
d(f^{-1})_y = (df_x)^{-1}
\]
where \(x = f^{-1}(y)\).

Now, by Cramers's rule, given an invertible matrix \(A\), the inverse is
\[
A^{-1} = \frac{1}{\det A} \operatorname{adj} A
\]
where the \(\operatorname{adj} A\) is the /adjugate matrix/ formed from cofactors of \(A\) - that is the determinants of the minors of \(A\).


As a function then, \(A \mapsto A^{-1}\) we see that the components are rational functions of the entries of \(A\) (since determinants are polynomials in the entries of \(A\)).
#+END_proof

** Inverse function theorem: Higher regularity

#+BEGIN_proof

Then the inverse function \(\operatorname{Inv}\) is in fact a smooth function from the open set of non-singular matrices (i.e. those with \(\det A \ne 0\)) to itself.

Then since \(x \mapsto df_x\) is smooth,
\[
y \mapsto df^{-1}_{f^{-1}(y)} = \operatorname{Inv} \circ df \circ f^{-1} (y)
\]
is the composition of \(C^0\) functions and hence \(df^{-1}\) is also \(C^0\).

That is \(f^{-1}\) is \(C^1\). Therefore in fact \(df^{-1}\) is the composition of \(C^1\) functions hence is also \(C^1\).

That is \(f^{-1}\) is \(C^2\). Now we just iterate to get \(f^{-1}\) is \(C^k\) for any \(k\) and hence smooth.
#+END_proof

